{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f1cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f30d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 定义标签和类别字典以及常量 N\n",
    "label_dict = {\"horse\": 0, \"ship\": 1, \"truck\": 2}\n",
    "class_dict = {0: \"horse\", 1: \"ship\", 2: \"truck\"}\n",
    "N = 3  # 总共三个类别\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, _data_dir, _transform, _loader):\n",
    "        # 获取标签文件夹列表\n",
    "        self.labels = [_label for _label in os.listdir(_data_dir)]\n",
    "        _file_path_label_list = [(os.path.join(_data_dir, _label, _img_fn), _label)\n",
    "                                 for _label in os.listdir(_data_dir)\n",
    "                                 for _img_fn in os.listdir(os.path.join(_data_dir, _label))\n",
    "                                 if not os.path.isdir(os.path.join(_data_dir, _label, _img_fn))]\n",
    "\n",
    "        self.data = [(_loader(_fp), label_dict[_label]) for _fp, _label in _file_path_label_list]\n",
    "        self.transform = _transform\n",
    "#         self.labels = [label for label in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, label))]\n",
    "        \n",
    "#         # 获取图像路径和标签的配对列表\n",
    "#         self.data = []\n",
    "#         for label in self.labels:\n",
    "#             label_path = os.path.join(data_dir, label)\n",
    "#             for img_fn in os.listdir(label_path):\n",
    "#                 img_path = os.path.join(label_path, img_fn)\n",
    "#                 if not os.path.isdir(img_path):\n",
    "#                     self.data.append((img_path, label_dict[label]))\n",
    "\n",
    "        # self.transform = transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        _img, _label = self.data[item]\n",
    "        _img = self.transform(_img)\n",
    "        return _img, _label\n",
    "        # img_path, label = self.data[index]\n",
    "        # img = Image.open(img_path).convert('RGB')\n",
    "        # if self.transform:\n",
    "        #     img = self.transform(img)\n",
    "        # return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def load_data():\n",
    "    print('data processing...')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # normalization\n",
    "    ])\n",
    "    data_dir = \"data/training_data/\"\n",
    "    train_dataset = MyDataset(data_dir, transform, _loader=lambda _path: Image.open(_path).convert('RGB'))\n",
    "    test_dataset = MyDataset(data_dir, transform, _loader=lambda _path: Image.open(_path).convert('RGB'))\n",
    "\n",
    "    train_size = int(len(train_dataset) * 0.8)\n",
    "    validate_size = len(train_dataset) - train_size\n",
    "    train, val = torch.utils.data.random_split(train_dataset, [train_size, validate_size])\n",
    "\n",
    "    train_data_loader = DataLoader(dataset=train, batch_size=50, shuffle=True, num_workers=0)\n",
    "    val_data_loader = DataLoader(dataset=val, batch_size=50, shuffle=True, num_workers=0)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=50, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_data_loader, val_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f25d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from classify.data_process import load_data, N, label_dict, class_dict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from classify.decorator import metric_time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(20)\n",
    "\n",
    "\n",
    "class cnn(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(3 * 3 * 64, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.out = nn.Linear(10, N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.out(x))\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_val_loss(model, Val):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    val_loss = []\n",
    "    for (data, target) in Val:\n",
    "        data, target = data.to(device), target.long().to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        val_loss.append(loss.cpu().item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "\n",
    "@metric_time\n",
    "def train():\n",
    "    train_data_loader, val_data_loader, _ = load_data()\n",
    "    print('train...')\n",
    "    epoch_num = 30\n",
    "    best_model = None\n",
    "    min_epochs = 5\n",
    "    min_val_loss = 5\n",
    "    model = cnn().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    for epoch in tqdm(range(epoch_num), ascii=True):\n",
    "        train_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
    "            data, target = data.to(device), target.long().to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.cpu().item())\n",
    "        # validation\n",
    "        val_loss = get_val_loss(model, val_data_loader)\n",
    "        if epoch + 1 > min_epochs and val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        tqdm.write('Epoch {:03d} train_loss {:.5f} val_loss {:.5f}'.format(epoch, np.mean(train_loss), val_loss))\n",
    "\n",
    "    torch.save(best_model.state_dict(), \"model/cnn.pkl\")\n",
    "\n",
    "\n",
    "@metric_time\n",
    "def test():\n",
    "    _, _, test_dataset = load_data()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = cnn().to(device)\n",
    "    model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n",
    "    total = 0\n",
    "    current = 0\n",
    "    model.eval()\n",
    "    for (data, target) in test_dataset:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        predicted = torch.max(outputs.data, 1)[1].data\n",
    "        total += target.size(0)\n",
    "        current += (predicted == target).sum()\n",
    "\n",
    "    print('Accuracy:%d%%' % (100 * current / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d20746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "942df1cd",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0cc888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing...\n",
      "train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|##7                                                                                | 1/30 [00:01<00:42,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train_loss 1.09396 val_loss 1.08727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|#####5                                                                             | 2/30 [00:02<00:41,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 train_loss 1.05307 val_loss 1.08259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|########3                                                                          | 3/30 [00:04<00:39,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 train_loss 1.01145 val_loss 1.06158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|###########                                                                        | 4/30 [00:05<00:38,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 train_loss 0.96858 val_loss 1.01852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|#############8                                                                     | 5/30 [00:07<00:36,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 train_loss 0.92605 val_loss 0.96698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|################6                                                                  | 6/30 [00:08<00:35,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 train_loss 0.90581 val_loss 0.94289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|###################3                                                               | 7/30 [00:10<00:33,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 train_loss 0.89011 val_loss 0.94442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|######################1                                                            | 8/30 [00:11<00:32,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 train_loss 0.85717 val_loss 0.90097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|########################9                                                          | 9/30 [00:13<00:31,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 train_loss 0.84862 val_loss 0.83560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|###########################3                                                      | 10/30 [00:14<00:29,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 train_loss 0.82518 val_loss 0.84286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|##############################                                                    | 11/30 [00:16<00:27,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 train_loss 0.81560 val_loss 0.84749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|################################8                                                 | 12/30 [00:17<00:26,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 train_loss 0.78825 val_loss 0.77784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|###################################5                                              | 13/30 [00:19<00:24,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 train_loss 0.77527 val_loss 0.77148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|######################################2                                           | 14/30 [00:20<00:23,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 train_loss 0.76654 val_loss 0.79574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|#########################################                                         | 15/30 [00:21<00:21,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 train_loss 0.74877 val_loss 0.80287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|###########################################7                                      | 16/30 [00:23<00:20,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 train_loss 0.74271 val_loss 0.78395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|##############################################4                                   | 17/30 [00:24<00:19,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 train_loss 0.73540 val_loss 0.70725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|#################################################1                                | 18/30 [00:26<00:17,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 train_loss 0.72971 val_loss 0.74335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|###################################################9                              | 19/30 [00:27<00:16,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 train_loss 0.72707 val_loss 0.76192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|######################################################6                           | 20/30 [00:29<00:14,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 train_loss 0.72190 val_loss 0.75744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|#########################################################4                        | 21/30 [00:30<00:13,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 train_loss 0.70127 val_loss 0.78170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|############################################################1                     | 22/30 [00:32<00:11,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 train_loss 0.69523 val_loss 0.73605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|##############################################################8                   | 23/30 [00:33<00:10,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 train_loss 0.68496 val_loss 0.72274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|#################################################################6                | 24/30 [00:35<00:09,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 train_loss 0.67922 val_loss 0.74310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|####################################################################3             | 25/30 [00:36<00:07,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 train_loss 0.66947 val_loss 0.70737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|#######################################################################           | 26/30 [00:38<00:05,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 train_loss 0.66965 val_loss 0.74213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|#########################################################################8        | 27/30 [00:39<00:04,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 train_loss 0.66549 val_loss 0.74045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|############################################################################5     | 28/30 [00:41<00:02,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 train_loss 0.65702 val_loss 0.70518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|###############################################################################2  | 29/30 [00:42<00:01,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 train_loss 0.64844 val_loss 0.69758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##################################################################################| 30/30 [00:44<00:00,  1.48s/it]\n",
      "\u001b[32m2024-11-28 16:55:05.846\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mclassify.decorator\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mtrain运行时间: 44.603296756744385 s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 train_loss 0.64894 val_loss 0.74360\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e002d",
   "metadata": {},
   "source": [
    "# 测试模型预测准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ceb88c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_6396/934605258.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n",
      "\u001b[32m2024-11-28 16:55:07.043\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mclassify.decorator\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mtest运行时间: 1.1858046054840088 s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:92%\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d7ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1fee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_6396/1498475470.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n"
     ]
    }
   ],
   "source": [
    "model = cnn().to(device)\n",
    "model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n",
    "model.eval()\n",
    "\n",
    "_img_path = \"data/testing_data/truck/truck-00028.jpg\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # normalization\n",
    "])\n",
    "img = Image.open(_img_path).convert('RGB')\n",
    "# 模拟批样本\n",
    "img_transform = transform(img).unsqueeze(0)\n",
    "\n",
    "output = model(img_transform)\n",
    "pred = class_dict[torch.max(output.data, 1)[1].data.item()]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f873ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2604d5-5150-414b-9ee4-09d55aaba8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
